{
    "title": "Garth statement",
    "slug": "garth-statement",
    "aliases": [
        "/Garth+statement",
        "/12020"
    ],
    "tiki_page_id": 12020,
    "date": "2020-10-22",
    "categories": [
        "Admin Only"
    ],
    "tags": [
        "Admin Only",
        "vitamin d"
    ]
}


<p>My tips for medical researchers:</p>

<p>1. <b>publish raw data</b> so anyone can reanalyze it if they don't agree with the stats.</p>

<p>2. <b>publish preprints</b> and embrace open science</p>

<p>3. <b>work with trained experimental scientists </b>if you/they are not your/themselves (e.g. doctors or nutritionists doing science) forming multidisciplinary coalitions as the quality benefits are massive from these collaborations.</p>

<p>4A. <b>Never ever ever use a p-value again</b>. (They don't mean what people think they do. They've done way more harm than good. They're an awful statistic and should be banned.)</p>

<p>4B.<b> Use confidence intervals. </b>And understand that a CI only represents 95% confidence at best, and because of type 1 and 2 errors they are usually <b>nowhere near 95% </b>in medicine. They should be interpreted with great care, <b>especially</b> if they are not clearly separated from comparison sets.</p>

<p>5. <b>Calculate power </b>before <i>and </i>after for <b>every </b>outcome including secondaries. Many studies think they're powered when they really are nowhere near (e.g. Martineau Mongolia which was just not a result, it wasn't a negative result. I calculated it had &lt;8% power for ARIs! The results were <i>meaningless</i>).</p>

<p>6. <b>List checks against all RCT biases </b>when discussing potential study weaknesses. </p>

<p>7. Write clearer, <b>more precise </b>abstract conclusion statements (if in doubt ask an experimental physicist for their opinion), far too many studies write categorically untrue abstract statements that simply are not justified by the data. Remember "insufficient evidence for X" tends to get interpreted as "evidence X doesn't work". These two statements are not equivalent.</p>

<p>8. <b>Plot all the data!! </b>Don't reduce complex stories to binary distinctions. A single statistic - by definition - hides all the information. Scatter plots are much better.</p>

<p>9. <b>Trends </b>are more useful than aggregates. (Given two numbers, one is generally going to be bigger... doesn't tell us much)</p>

<p>10. NEVER ADJUST FOR CONFOUNDERS <b>without a causal model</b>. You could well be making the bias worse not better (e.g. Hastie).</p>

<p>11. NEVER ADJUST FOR BIAS <b>in small data sets </b>(N &lt; 1000) even with a causal model. Again, you risk introducing bias. One of the mathematical requirements for typical bias adjustments is the assumption of large evenly distributed data that behaves well. This isn't met if your data set is small. <b><u>Anything could happen!</u></b></p>

<p>12. In e.g. vitamin D, don't use defined thresholds for deficiency. <b>Plot the actual serum values.</b></p>

<p><b>13. </b>In large data sets, <b>causal correlations by far exceed in number spurious ones (correlation IS causation - mostly). </b>In small data, the opposite is true. Intuitions from working with small numbers of patients don't transfer to big data but most doctors seem to do this. Common (distal) causes should be obvious if present. If not, draw causal models (DAGs) and <b>think through what could be happening </b>and look at other confounder types. The fact that vitamin D was considered a potential 'bystander' confounder for so many decades astonishes me. It's just so obviously not. (Read the Book of Why by Judea Pearl as a starting point - it's a great lay introduction to causal inference).</p>

<p>14. Remember: <b>any single RCT is meaningless </b>no matter the size because of potential bias/method flaws. Analysis of cohorts of studies preferably large observational combined with RCTs will give a much clearer and more reliable picture.</p>

<p>https://mail.google.com/mail/u/0/#inbox/KtbxLxgFzXhCCnPdbKnsFZZLWQlvhjHDqB  Oct 22</p>